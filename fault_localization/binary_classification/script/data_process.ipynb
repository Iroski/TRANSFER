{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import javalang\n",
    "import pickle\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_camel_and_underline(token):\n",
    "    if token.isdigit():\n",
    "        return [token]\n",
    "    else:\n",
    "        p = re.compile(r'([a-z]|\\d)([A-Z])')\n",
    "        sub = re.sub(p, r'\\1_\\2', token).lower()\n",
    "        sub_tokens = sub.split(\"_\")\n",
    "        tokens = re.sub(\" +\", \" \", \" \".join(sub_tokens)).strip()\n",
    "        final_token = []\n",
    "        for factor in tokens.split(\" \"):\n",
    "            final_token.append(factor.rstrip(string.digits))\n",
    "        return final_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_data(token_seq, token_length_for_reserve):\n",
    "    if len(token_seq) <= token_length_for_reserve:\n",
    "        return token_seq\n",
    "    else:\n",
    "        start_index = token_seq.index(\"rank2fixstart\")\n",
    "        end_index = token_seq.index(\"rank2fixend\")\n",
    "        assert end_index > start_index\n",
    "        length_of_annotated_statement = end_index - start_index + 1\n",
    "        if length_of_annotated_statement <= token_length_for_reserve:\n",
    "            padding_length = token_length_for_reserve - length_of_annotated_statement\n",
    "            # give 2/3 padding space to content before annotated statement\n",
    "            pre_padding_length = padding_length // 3 * 2\n",
    "            # give 1/3 padding space to content after annotated statement\n",
    "            post_padding_length = padding_length - pre_padding_length\n",
    "            if start_index >= pre_padding_length and len(token_seq) - end_index - 1 >= post_padding_length:\n",
    "                return token_seq[start_index - pre_padding_length: end_index + 1 + post_padding_length]\n",
    "            elif start_index < pre_padding_length:\n",
    "                return token_seq[:token_length_for_reserve]\n",
    "            elif len(token_seq) - end_index - 1 < post_padding_length:\n",
    "                return token_seq[len(token_seq) - token_length_for_reserve:]\n",
    "        else:\n",
    "            return token_seq[start_index: start_index + token_length_for_reserve]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token_seq(tokens,token_length_for_reserve):\n",
    "\tlast_token=None\n",
    "\tt_seq = []\n",
    "\tfor token in tokens:\n",
    "\t\tif isinstance(token, javalang.tokenizer.String):\n",
    "\t\t\ttmp_token = [\"stringliteral\"]\n",
    "\t\telse:\n",
    "\t\t\ttmp_token = solve_camel_and_underline(token.value)\n",
    "\t\t\tif last_token is not None and isinstance(token, javalang.tokenizer.Identifier) and (\n",
    "\t\t\t\t\tisinstance(last_token, javalang.tokenizer.Identifier) or isinstance(last_token,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   javalang.tokenizer.BasicType)) and last_token.value!='rank2fixstart':\n",
    "\t\t\t\tcur_token=tmp_token\n",
    "\t\t\t\ttmp_token=[]\n",
    "\t\t\t\ttype_token_seq = solve_camel_and_underline(last_token.value)\n",
    "\t\t\t\tfor t_token in cur_token:\n",
    "\t\t\t\t\ttmp_token.append([type_token_seq, cur_token[:cur_token.index(t_token)], t_token])\n",
    "\t\tlast_token=token\n",
    "\t\tt_seq += tmp_token\n",
    "\treturn cut_data(t_seq, token_length_for_reserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2num(tokens,vocab_dict,index_oov):\n",
    "    num_seq=[]\n",
    "    for token in tokens:\n",
    "        if type(token)==str:\n",
    "            num=vocab_dict[token] if token in vocab_dict else index_oov\n",
    "        else:\n",
    "\n",
    "            identifiers=token[0]\n",
    "            prev_token=token[1]\n",
    "            cur_token=token[2]\n",
    "            iden_nums=[vocab_dict[t] if token in vocab_dict else index_oov for t in identifiers]\n",
    "            prev_nums=[vocab_dict[t] if token in vocab_dict else index_oov for t in prev_token]\n",
    "            cur_num=vocab_dict[cur_token] if token in vocab_dict else index_oov\n",
    "            num=[iden_nums,prev_nums,cur_num]\n",
    "        num_seq.append(num)\n",
    "    return num_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters declaration\n"
     ]
    }
   ],
   "source": [
    "# Parameters declaration\n",
    "print(\"Parameters declaration\")\n",
    "tags = [\"positive\", \"negative\"]\n",
    "train_prop = 0.8\n",
    "val_prop = 0.1\n",
    "test_prop = 0.1\n",
    "shuffle_seed = 666\n",
    "vector_size = 32\n",
    "token_length_for_reserve = 400\n",
    "extend_size = 2\n",
    "max_vocab_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path declaration\n"
     ]
    }
   ],
   "source": [
    "current_pattern='InsertMissedStmt'\n",
    "print(\"Path declaration\")\n",
    "fl_dataset_path = \"../../dataset_fl.pkl\"\n",
    "output_data_dir = \"../data/{}/\".format(current_pattern)\n",
    "if not os.path.exists(output_data_dir):\n",
    "    os.makedirs(output_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/val/test parts and generate training corpus for word2vec pre-training\n",
    "with open(fl_dataset_path, \"rb\") as file:\n",
    "    dataset_fl = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Override public void onError(Throwable t){\n",
      "  if (done) {\n",
      " rank2fixstart     return; rank2fixend \n",
      "  }\n",
      "  done=true;\n",
      "  if (s == null) {\n",
      "    CompositeException t2=new CompositeException(t,new NullPointerException(\"Subscription not set!\"));\n",
      "    try {\n",
      "      actual.onSubscribe(EmptyDisposable.INSTANCE);\n",
      "    }\n",
      " catch (    Throwable e) {\n",
      "      Exceptions.throwIfFatal(e);\n",
      "      t2.suppress(e);\n",
      "      RxJavaPlugins.onError(t2);\n",
      "      return;\n",
      "    }\n",
      "    try {\n",
      "      actual.onError(t2);\n",
      "    }\n",
      " catch (    Throwable e) {\n",
      "      Exceptions.throwIfFatal(e);\n",
      "      t2.suppress(e);\n",
      "      RxJavaPlugins.onError(t2);\n",
      "    }\n",
      "    return;\n",
      "  }\n",
      "  if (t == null) {\n",
      "    t=new NullPointerException();\n",
      "  }\n",
      "  try {\n",
      "    actual.onError(t);\n",
      "  }\n",
      " catch (  Throwable ex) {\n",
      "    Exceptions.throwIfFatal(ex);\n",
      "    RxJavaPlugins.onError(new CompositeException(t,ex));\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "one_method=dataset_fl['positive'][current_pattern][10]\n",
    "print(one_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "method=one_method.strip()\n",
    "tokens=javalang.tokenizer.tokenize(method)\n",
    "token_seq=generate_token_seq(tokens,token_length_for_reserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'override', 'public', 'void', 'on', 'error', '(', 'throwable', [['throwable'], [], 't'], ')', '{', 'if', '(', 'done', ')', '{', 'rank2fixstart', 'return', ';', 'rank2fixend', '}', 'done', '=', 'true', ';', 'if', '(', 's', '==', 'null', ')', '{', 'composite', 'exception', [['composite', 'exception'], [], 't'], '=', 'new', 'composite', 'exception', '(', 't', ',', 'new', 'null', 'pointer', 'exception', '(', 'stringliteral', ')', ')', ';', 'try', '{', 'actual', '.', 'on', 'subscribe', '(', 'empty', 'disposable', '.', 'instance', ')', ';', '}', 'catch', '(', 'throwable', [['throwable'], [], 'e'], ')', '{', 'exceptions', '.', 'throw', 'if', 'fatal', '(', 'e', ')', ';', 't', '.', 'suppress', '(', 'e', ')', ';', 'rx', 'java', 'plugins', '.', 'on', 'error', '(', 't', ')', ';', 'return', ';', '}', 'try', '{', 'actual', '.', 'on', 'error', '(', 't', ')', ';', '}', 'catch', '(', 'throwable', [['throwable'], [], 'e'], ')', '{', 'exceptions', '.', 'throw', 'if', 'fatal', '(', 'e', ')', ';', 't', '.', 'suppress', '(', 'e', ')', ';', 'rx', 'java', 'plugins', '.', 'on', 'error', '(', 't', ')', ';', '}', 'return', ';', '}', 'if', '(', 't', '==', 'null', ')', '{', 't', '=', 'new', 'null', 'pointer', 'exception', '(', ')', ';', '}', 'try', '{', 'actual', '.', 'on', 'error', '(', 't', ')', ';', '}', 'catch', '(', 'throwable', [['throwable'], [], 'ex'], ')', '{', 'exceptions', '.', 'throw', 'if', 'fatal', '(', 'ex', ')', ';', 'rx', 'java', 'plugins', '.', 'on', 'error', '(', 'new', 'composite', 'exception', '(', 't', ',', 'ex', ')', ')', ';', '}', '}']\n"
     ]
    }
   ],
   "source": [
    "print(token_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load vocab and vectors\n"
     ]
    }
   ],
   "source": [
    "print(\"load vocab and vectors\")\n",
    "with open(os.path.join(output_data_dir, \"vocab.pkl\"), \"rb\") as file:\n",
    "    vocab_dict=pickle.load(file)\n",
    "with open(os.path.join(output_data_dir, \"vectors.pkl\"), \"rb\") as file:\n",
    "    vectors= pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train/val/test dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating train/val/test dataset\")\n",
    "index_oov = vocab_dict[\"OOV\"]\n",
    "index_padding = vocab_dict[\"PADDING\"]\n",
    "current_index = len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_record = token2num(token_seq,vocab_dict,index_oov)\n",
    "if len(normal_record) < token_length_for_reserve:\n",
    "    normal_record += [index_padding] * (token_length_for_reserve - len(normal_record))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_seq_dataset = {\"train\": {}, \"val\": {}, \"test\": {}}\n",
    "w2v_training_corpus = []\n",
    "for tag in dataset_fl:\n",
    "    for part in token_seq_dataset:\n",
    "        token_seq_dataset[part][tag] = []\n",
    "    random_copy = dataset_fl[tag][current_pattern][:]\n",
    "    random.seed(shuffle_seed)\n",
    "    random.shuffle(random_copy)\n",
    "    for index, method in enumerate(random_copy):\n",
    "        method = method.strip()\n",
    "        tokens = javalang.tokenizer.tokenize(method)\n",
    "\n",
    "        token_seq = generate_token_seq(tokens,token_length_for_reserve)\n",
    "\n",
    "        if index + 1 < len(random_copy) * train_prop:\n",
    "            token_seq_dataset[\"train\"][tag].append(token_seq)\n",
    "        elif index + 1 < len(random_copy) * (train_prop + val_prop):\n",
    "            token_seq_dataset[\"val\"][tag].append(token_seq)\n",
    "        else:\n",
    "            token_seq_dataset[\"test\"][tag].append(token_seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('transfer': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22fcc13e4ce33af54b9bfdefb95036e9d5ed01c6c88ad48fe844413e6447d380"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
